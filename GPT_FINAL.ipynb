{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNLYyTifbhPiwb1AaS3oQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktornetta/dad_jokes/blob/main/GPT_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVdpaVQiYfHh"
      },
      "source": [
        "# **Generating Dad Jokes with GPT-2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CAc7E5i3W9O"
      },
      "source": [
        "**Aray Almen, Kelly Tornetta, Marissa Whitby**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE_gj2t0eGfw"
      },
      "source": [
        "We are utilizing Google Colab's free GPU to train a small GPT-2 model on a database of dad jokes, the Dadabase. After training, we will be able to generate original dad jokes based on different parameters. Note that this notebook should be run with Google Chrome and Google Colab because we will be using Google specific features. We hope you enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgZKVi16x1A_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcc-BBPWx5AN"
      },
      "source": [
        "Before fine-tuning the GPT-2 model, we need to setup the notebook to run all of our code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inHWQ29_xNjr"
      },
      "source": [
        "### Download Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWi8L2nfcOfB"
      },
      "source": [
        "First, download the necessary libraries. These are:\n",
        "\n",
        "\n",
        "*   TensorFlow 1.5 \n",
        "  * need previous version with tensorflow.contrib module that is removed in current update to run GPT-2 model\n",
        "*   Files from Google Colab\n",
        "  * allows us to import .csv dataframe and export .txt output jokes\n",
        "*   GPT-2 from [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)\n",
        "  * free Python package for downloading and finetuning a GPT-2 model by [OpenAI](https://openai.com/blog/better-language-models/) and [Neil Shepperd](https://github.com/nshepperd/gpt-2) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkfMSltfZNxC",
        "outputId": "58207887-238d-4880-a42c-b7bbc5873f31"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeXMU0dGfzYH",
        "outputId": "5e25fe65-4380-4af4-c3a9-887bd7df7e59"
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be-TsSklxbiI"
      },
      "source": [
        "### Active GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x6ARhKVf-xp"
      },
      "source": [
        "We can verify which GPU is active in Colab. Note that Colab either uses an Nvidia T4 GPU or an Nvidia K80 GPU. If you are training a larger GPT-2 model, it is recommended that the T4 is used since it is slightly faster than the older K80. Since we are using the smallest GPT-2 model, the GPU doesn't matter too much. \n",
        "\n",
        "If you see the error: \"NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.\" then you need to manually select the GPU by choosing Edit -> Notebook settings -> Hardware accelerator -> GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXp3X6oqf4MJ",
        "outputId": "6e503102-e4dd-47d5-e043-dd125fdb9b1d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 16 17:25:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     8W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID0xMyYHxhIf"
      },
      "source": [
        "### Download GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYceeQQahWg1"
      },
      "source": [
        "Download the version of GPT-2 that you wish to fine-tune. For the sake of run time and size, we are using the smallest 124M GPT-2 model. If you wish to train a larger model with a large amount of training data, gpt-2-simple can also support the fine-tuning of a 355M model of GPT-2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwBuRZJhggcg",
        "outputId": "d362b2e4-6a6e-49ca-b86e-ef8d9aa1a2f1"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 242Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 381Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 174Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 377Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 176Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 186Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVUSxgMExnPp"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88lRU7RRjAM5"
      },
      "source": [
        "Since we are using Google Colab, we also have access to Google Drive. Mounting our personal Google Drive in the VM allows easy transfer of data in/out of the VM. This means that we only have to train the GPT-2 model with the Dadabase once, and then can upload this trained model to generate jokes without having to re-run the fine-tuning. You will have to authorize this via a url to your google account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joy85CTkie4_",
        "outputId": "0cf395cd-f144-46e1-caed-e9c12a1cd168"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m31nOASsyUQ3"
      },
      "source": [
        "## Import Data to Fine-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdFvx9K6kAcI"
      },
      "source": [
        "Upload the Dadabase via the left sidebar (file logo) Files -> (paper with upwards arrow) Upload -> select the Dadabase.csv from your local device or drag and drop into the module. You could also upload via typing the command *uploaded = files.upload()* and then selecting the file. Once uploaded, we can call the file and import into the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd2NKhmwj2q1"
      },
      "source": [
        "file_name = \"Dadabase.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPlO10hXyato"
      },
      "source": [
        "## Fine-tune GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHBF9vw4yums"
      },
      "source": [
        "Fine-tune the GPT-2 Model with the Dadabase. This can be done multiple times by saving different checkpoints with different parameters. We have included our final chosen model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P56uXinVymNX"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otaItUzxlAhR"
      },
      "source": [
        "We have selected the following parameters for our final model:\n",
        "* *model_name = '124M'* - the smallest model of GPT-2\n",
        "* *steps = 500* - small number of steps to train since short form text is more likely to overfit\n",
        "* *restore_from = 'fresh'* - training from the base GPT-2, choose *'latest'* to restore from a previously trained model\n",
        "* *run_name = 'dadjokes1'* - saves the model in folder 'dadjokes1' within the folder 'checkpoint' \n",
        "* *print_every = 10* - prints every 10 steps during the training process\n",
        "* *sample_every = 100* - prints 100 example outputs per step\n",
        "* *learning_rate = 1e-4* - default learning rate\n",
        "\n",
        "Using these parameters, the training process took about 20 minutes. Note that increasing the number of steps increases the amount of time to train, with 1000 steps taking about an hour to train.*italicized text*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa1QJ9kylNzi",
        "outputId": "9429896f-8ab9-4501-ceee-668c34130910"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=500,\n",
        "              restore_from='fresh',\n",
        "              run_name='dadjokes1',\n",
        "              print_every=10,\n",
        "              sample_every=100\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 219.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "dataset has 29571 tokens\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10 | 28.78] loss=1.74 avg=1.74\n",
            "[20 | 50.41] loss=1.53 avg=1.63\n",
            "[30 | 72.27] loss=1.26 avg=1.51\n",
            "[40 | 94.32] loss=1.13 avg=1.41\n",
            "[50 | 116.59] loss=0.88 avg=1.30\n",
            "[60 | 139.06] loss=0.46 avg=1.16\n",
            "[70 | 161.67] loss=0.38 avg=1.04\n",
            "[80 | 184.49] loss=0.32 avg=0.95\n",
            "[90 | 207.58] loss=0.24 avg=0.87\n",
            "[100 | 230.64] loss=0.21 avg=0.80\n",
            "======== SAMPLE 1 ========\n",
            "text|>\n",
            "<|endoftext|>\n",
            "<|startoftext|>Why did the elf cross the road? With a cross<|endoftext|>\n",
            "<|startoftext|>What’s Fifty Cent’s name in Zimbabwe? Two Hundred Dollars.<|endoftext|>\n",
            "<|startoftext|>Why did the pig go to the doctor? It needed something to eat.<|endoftext|>\n",
            "<|startoftext|>What’s Fifty Cent’s favorite song to swing is \"Can I Have Three Chairs? There are three Chairs.\"<|endoftext|>\n",
            "<|startoftext|>What do you call a bald porcupine? Kudzu.<|endoftext|>\n",
            "<|startoftext|>What does a scientist do on a roller coaster? It's nobody else's business.<|endoftext|>\n",
            "<|startoftext|>Why did the apple fall over? Because it was in the wrong place.<|endoftext|>\n",
            "<|startoftext|>Why did the girl take the banana to the police? They found it odd that she was wearing a purse.<|endoftext|>\n",
            "<|startoftext|>Why did the pot call the police? They'd be a trouble-stack.<|endoftext|>\n",
            "<|startoftext|>Why did the girl quit her job at the donut factory? She was fed up with the hole business.<|endoftext|>\n",
            "<|startoftext|>Why did the boy take the apple to the ointment? He needed something to eat.<|endoftext|>\n",
            "<|startoftext|>Why did the scarf fall over? It was a scarf.<|endoftext|>\n",
            "<|startoftext|>Why did the broom go to the bath? It was looking for a deWitt.<|endoftext|>\n",
            "<|startoftext|>Why did the paper follow the pencil? Because it lead the way.<|endoftext|>\n",
            "<|startoftext|>Why did the boy quit his job at the newsstand? He was looking for a other career.<|endoftext|>\n",
            "<|startoftext|>Why did the pile of dishes get in the way of making sandwiches? They got in too many places.<|endoftext|>\n",
            "<|startoftext|>Why did the pony get into the game of catch? He was caught in a catch.<|endoftext|>\n",
            "<|startoftext|>Why did the wheat get the most ire? It was getting so big.<|endoftext|>\n",
            "<|startoftext|>Why did the baby corn get a big enough hug? I don’t see why it couldn't get one.<|endoftext|>\n",
            "<|startoftext|>Why did the boy take the lollipop to lunch? He got it for lunch.<|endoftext|>\n",
            "<|startoftext|>Why did the boy take the ladder to the hospital? He got it to go to the ICU.<|endoftext|>\n",
            "<|startoftext|>Why did the green tomato get arrested? It was arrested for grow-thru.<|endoftext|>\n",
            "<|startoftext|>Why did the boy take the knife to school? He cut it down.<|endoftext|>\n",
            "<|startoftext|>Why did the melons job get cut? They made a big pile.<|endoftext|>\n",
            "<|startoftext|>Why did balloons go to the doctor? They needed to take a nap.<|endoftext|>\n",
            "<|startoftext|>Why did the kid get fired from the restaurant? He got fired from the pizza restaurant.<|endoftext|>\n",
            "<|startoftext|>Why did the melons job get fired? It got fired from the job.<|endoftext|>\n",
            "<|startoftext|>Why did the grape get fired? It was grape.<|endoftext|>\n",
            "<|startoftext|>Why did the kid get fired from the playground? He got fired from the playground.<|endoftext|>\n",
            "<|startoftext|>Why did the car get towed? Because it was impounded.<|endoftext|>\n",
            "<|startoftext|>Why did the girl get fired from school? She was bad at school.<|endoftext|>\n",
            "<|startoftext|>Why did the boy take the pencil to bed? He used it as a pad for hectic studies.<|endoftext|>\n",
            "<|startoftext|>Why did Mrs.\n",
            "\n",
            "[110 | 265.49] loss=0.18 avg=0.74\n",
            "[120 | 288.91] loss=0.11 avg=0.68\n",
            "[130 | 312.41] loss=0.09 avg=0.63\n",
            "[140 | 335.98] loss=0.08 avg=0.59\n",
            "[150 | 359.86] loss=0.08 avg=0.56\n",
            "[160 | 383.76] loss=0.07 avg=0.52\n",
            "[170 | 407.62] loss=0.05 avg=0.49\n",
            "[180 | 431.58] loss=0.04 avg=0.47\n",
            "[190 | 455.62] loss=0.04 avg=0.44\n",
            "[200 | 479.57] loss=0.02 avg=0.42\n",
            "======== SAMPLE 1 ========\n",
            "||startoftext|>Why couldn't the lifeguard save the hippie? He was too far out.<|endoftext|>\n",
            "<|startoftext|>Why are basketball players so slow to apologize? Because they're so far removed.<|endoftext|>\n",
            "<|startoftext|>Why are the hotties jerks? They take something very seriously.<|endoftext|>\n",
            "<|startoftext|>Why are homeroom players at all? Because they're jerks.<|endoftext|>\n",
            "<|startoftext|>Why are superdelegates to the basketball camps? They are superdelegates.<|endoftext|>\n",
            "<|startoftext|>Why are all-star-caliber forwards? Because they're all-star caliber.<|endoftext|>\n",
            "<|startoftext|>Why are all-star-quality bats? Because they're all-star-quality.<|endoftext|>\n",
            "<|startoftext|>Why are all-star-talented golf players? They all-star-learned players.<|endoftext|>\n",
            "<|startoftext|>Why are all-star-talented artists? They all-star-artists.<|endoftext|>\n",
            "<|startoftext|>Why are all-star-talented doctors? They all-star-maine patients.<|endoftext|>\n",
            "<|startoftext|>Why are all-star batteries so weak? Because they're weak batteries.<|endoftext|>\n",
            "<|startoftext|>Why are ships so slow to apologize? Because they're so far from the straits.<|endoftext|>\n",
            "<|startoftext|>Why are proteins so quick to apologize? Because they're proteins.<|endoftext|>\n",
            "<|startoftext|>Why are the evils of discord so easy to get along with? Because of their looks.<|endoftext|>\n",
            "<|startoftext|>Why are pirates so mean? Because they are pirates.<|endoftext|>\n",
            "<|startoftext|>Why are proteins so slow to apologize? Because they're proteins.<|endoftext|>\n",
            "<|startoftext|>Why are the foals so mean? Because they were meant to be a-beasts.<|endoftext|>\n",
            "<|startoftext|>Why are the jerks in the news? They're the only ones facing the day in court.<|endoftext|>\n",
            "<|startoftext|>I hate people who flatter me. They're just jerks.<|endoftext|>\n",
            "<|startoftext|>What do you call a bald porcupine? Pointless.<|endoftext|>\n",
            "<|startoftext|>What did the owner of a brownie factory say when his factory caught fire?  I'm getting the fudge outta here.<|endoftext|>\n",
            "<|startoftext|>Why are cats bad liars? Because they recoils.<|endoftext|>\n",
            "<|startoftext|>What do you do if a butcher says yes on a bet? I'll give you a little cash.<|endoftext|>\n",
            "<|startoftext|>You know that feeling you get when you see all the cheese on the table? A little fluff.<|endoftext|>\n",
            "<|startoftext|>What do you get when you cross a chicken with a ukulele? A thigh-raisin.<|endoftext|>\n",
            "<|startoftext|>What do ducks do? They get outed.<|endoftext|>\n",
            "<|startoftext|>What do you get when you go to the zoo? Lots of maws.<|endoftext|>\n",
            "<|startoftext|>What do you call a bear with no teeth? A gummy bear.<|endoftext|>\n",
            "<|startoftext|>What do you say when you get an E? Between the ears, it's a E.<|endoftext|>\n",
            "<|startoftext|>What do you get when you give animals alcohol? An icy drink.<|endoftext|>\n",
            "<|startoftext|>What do you call a cow with a mouthful of corn? A milk shake.<|endoftext|>\n",
            "<|startoftext|>What do you call a con-artist who minored in psychology? Sigmund Fraud.<|endoftext|>\n",
            "<|startoftext|>What do you call the mouthwash program? MS-097. The fish is the talk.<|endoftext|>\n",
            "<|start\n",
            "\n",
            "[210 | 513.87] loss=0.03 avg=0.40\n",
            "[220 | 537.79] loss=0.03 avg=0.38\n",
            "[230 | 561.82] loss=0.02 avg=0.36\n",
            "[240 | 585.88] loss=0.03 avg=0.35\n",
            "[250 | 609.90] loss=0.02 avg=0.33\n",
            "[260 | 633.88] loss=0.02 avg=0.32\n",
            "[270 | 657.88] loss=0.03 avg=0.31\n",
            "[280 | 681.83] loss=0.03 avg=0.29\n",
            "[290 | 705.80] loss=0.01 avg=0.28\n",
            "[300 | 729.75] loss=0.02 avg=0.27\n",
            "======== SAMPLE 1 ========\n",
            "set>I'll’t you mind going by myself?<|endoftext|>\n",
            "<|startoftext|>Today a girl said she recognized me from vegetarian club, but I’m sure I’ve never met herbivore.<|endoftext|>\n",
            "<|startoftext|>How do you organize a space party? You planet.<|endoftext|>\n",
            "<|startoftext|>How do you make holy water? You boil the hell out of it.<|endoftext|>\n",
            "<|startoftext|>They laughed when I said I wanted to be a comedian. They’re not laughing now.<|endoftext|>\n",
            "<|startoftext|>What does an angry pepper do? It gets jalapeño face.<|endoftext|>\n",
            "<|startoftext|>Don't buy flowers at a monastery. Because only you can prevent florists.<|endoftext|>\n",
            "<|startoftext|>Do you have a preference of where you sit? Down.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the submarine industry? It really took a dive.<|endoftext|>\n",
            "<|startoftext|>To the person who stole my anti-depressant pills, I hope you're happy now.<|endoftext|>\n",
            "<|startoftext|>How do you get a baby alien to sleep? You rocket.<|endoftext|>\n",
            "<|startoftext|>Why do pirates not know the alphabet? They always get stuck at \"C\".<|endoftext|>\n",
            "<|startoftext|>Why are teachers happy at Halloween parties? Because there is lots of school spirit!<|endoftext|>\n",
            "<|startoftext|>I.P. chips. Lots of chips.<|endoftext|>\n",
            "<|startoftext|>I.P. chips. Childhood candy. A school snack.<|endoftext|>\n",
            "<|startoftext|>I.P. chips. I.P. breakfast. Meet you later classmate.<|endoftext|>\n",
            "<|startoftext|>I.P. chips. I'm hungry!<|endoftext|>\n",
            "<|startoftext|>Why did the house go to the doctor? It was having window panes.<|endoftext|>\n",
            "<|startoftext|>I.P. chips. I was sick when the filler was removed.<|endoftext|>\n",
            "<|startoftext|>I.P. chips. I'm sorry, I can't take it.<|endoftext|>\n",
            "<|startoftext|>I'm Breathe. Breathe well.<|endoftext|>\n",
            "<|startoftext|>What do you call a mummy? An Invisigoth.<|endoftext|>\n",
            "<|startoftext|>Iced eels. I fear they're destined to revolt.<|endoftext|>\n",
            "<|startoftext|>What do you call a cow with no guts? Ground beef.<|endoftext|>\n",
            "<|startoftext|>What do you give a sick puppy? A Coke.<|endoftext|>\n",
            "<|startoftext|>I couldn't figure out how the seat belt worked. Then it just clicked.<|endoftext|>\n",
            "<|startoftext|>Why was the chin injured? It sent a shiver up my spine.<|endoftext|>\n",
            "<|startoftext|>What do you call a dumb puppy? A Deaf puppy.<|endoftext|>\n",
            "<|startoftext|>Why did the teddy bear get hit by a car? She was a little tough tack.<|endoftext|>\n",
            "<|startoftext|>I asked the cashier if I could take the money in, but the cashier said, sorry, you don<|endoftext|>\n",
            "<|startoftext|>I’m just adding to the pile. Should take some explaining to do.<|endoftext|>\n",
            "<|startoftext|>What did the hat say to the scarf? You can hang around, I'll just go on ahead.<|endoftext|>\n",
            "<|startoftext|>I know a Brooklyn beekeeper that sells balloon service. He's the only broker in the area.<|endoftext|>\n",
            "<|startoftext|>What do you call a bee that lives in a dark basement? A library-room.<|endoftext|>\n",
            "<|startoftext|>I asked the cheese what it was made from, and it said . .\n",
            "\n",
            "[310 | 764.10] loss=0.02 avg=0.26\n",
            "[320 | 788.02] loss=0.03 avg=0.26\n",
            "[330 | 811.93] loss=0.02 avg=0.25\n",
            "[340 | 835.87] loss=0.02 avg=0.24\n",
            "[350 | 859.77] loss=0.02 avg=0.23\n",
            "[360 | 883.71] loss=0.02 avg=0.22\n",
            "[370 | 907.66] loss=0.02 avg=0.22\n",
            "[380 | 931.61] loss=0.02 avg=0.21\n",
            "[390 | 955.51] loss=0.02 avg=0.21\n",
            "[400 | 979.44] loss=0.03 avg=0.20\n",
            "======== SAMPLE 1 ========\n",
            " a word. Therehe has it.<|endoftext|>\n",
            "<|startoftext|>\"Dad, I'm cold.\" \"Go stand in the corner, I hear it's 90 degrees.\"<|endoftext|>\n",
            "<|startoftext|>\"Dad, make me a sandwich.\" \"Poof! You're a sandwich.\"<|endoftext|>\n",
            "<|startoftext|>Why aren't koalas actual bears? They dont meet the koalafications.<|endoftext|>\n",
            "<|startoftext|>Why are jokes about rotten eggs banned? Because they're infeggtious.<|endoftext|>\n",
            "<|startoftext|>Why are horses never overweight? They're on a stable diet.<|endoftext|>\n",
            "<|startoftext|>Why are giraffes slow to apologize? It takes them a long time to swallow their pride.<|endoftext|>\n",
            "<|startoftext|>I like my jokes they way I like my robots. Killer.<|endoftext|>\n",
            "<|startoftext|>I like camping, but it's so in tents.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the butcher who backed into the meat grinder? He got a little behind in his work.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the Antennas that got married? The wedding was lame, but the reception was great.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the production delays at that company that makes scales using lengthy pipes? They had really long weights.<|endoftext|>\n",
            "<|startoftext|>Did you hear about what happened with the elk? It was really amoosing.<|endoftext|>\n",
            "<|startoftext|>Did you take a shower today? Why, is one missing?<|endoftext|>\n",
            "<|startoftext|>Did you hear about the fight in the candy store? Two suckers got licked.<|endoftext|>\n",
            "<|startoftext|>Did you hear about Scrooge's drinking problem? He had a dickens of a time with spirits.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the man who was accidentally buried alive? It was a grave mistake. <|endoftext|>\n",
            "<|startoftext|>Did you hear about the two monocles at the party? They made spectacles out of themselves.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the guy who invented a knife that can cut four loaves of bread at once? He's calling it the \"Four Loaf Cleaver.\"<|endoftext|>\n",
            "<|startoftext|>Did you hear about the lawyer for U2? He was Pro-Bono.<|endoftext|>\n",
            "<|startoftext|>Did you hear that H.P. Lovecraft wrote a cookbook? It's called the Necronomnom.<|endoftext|>\n",
            "<|startoftext|>Did you know that in high school, Robert E. Lee was voted \"most likely to secede?\"<|endoftext|>\n",
            "<|startoftext|>Did you hear about the two lawyers who set up shop under the old oak tree? I heard it was a pretty shady business.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the two silk worms that got in a fight? It ended in a tie.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the corduroy pillow? It made headlines.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the neutron who was arrested? He was held without charge.<|endoftext|>\n",
            "<|startoftext|>Did you know that it's traditional to serve Eggs Benedict on a hubcap? There's no plate like chrome for the Hollandaise.<|endoftext|>\n",
            "<|startoftext|>Did you hear the joke about the fast car? I would tell you but I think you're too slow to get it.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the Italian chef that died? He pasta way.<|endoftext|>\n",
            "<|startoftext|>Did you hear about the mathematician who hated negative numbers? He'll stop at nothing to avoid them.<|endoftext|>\n",
            "<|startoftext|>Did you hear about that spicy knight? Sir Acha.<|endoftext|>\n",
            "<|startoftext|\n",
            "\n",
            "[410 | 1013.78] loss=0.02 avg=0.20\n",
            "[420 | 1037.69] loss=0.02 avg=0.19\n",
            "[430 | 1061.58] loss=0.02 avg=0.19\n",
            "[440 | 1085.51] loss=0.02 avg=0.18\n",
            "[450 | 1109.44] loss=0.02 avg=0.18\n",
            "[460 | 1133.36] loss=0.01 avg=0.17\n",
            "[470 | 1157.30] loss=0.02 avg=0.17\n",
            "[480 | 1181.25] loss=0.01 avg=0.16\n",
            "[490 | 1205.22] loss=0.01 avg=0.16\n",
            "[500 | 1229.20] loss=0.02 avg=0.16\n",
            "Saving checkpoint/dadjokes1/model-500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27qM5un1zGGK"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5WIYDWkrI0e"
      },
      "source": [
        "Since this model has been checkpointed and saved, we can download it to our Google Drive using the command below and then reload the trained model using the command *gpt2.copy_checkpoint_from_gdrive(run_name = 'dadjokes1')*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EtfFB8JlR8w"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='dadjokes1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cACIoPWLzJPi"
      },
      "source": [
        "## Generate Jokes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWr0RXE_zthW"
      },
      "source": [
        "Now, it's time to finally generate jokes from our fine-tuned model! If you are using a previously trained model, uncomment and run the code below with the desired model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-LOzhz_z-WW"
      },
      "source": [
        "#gpt2.copy_checkpoint_from_gdrive(run_name = 'dadjokes1')\n",
        "#sess = gpt2.start_tf_sess()\n",
        "#gpt2.load_gpt2(sess, run_name = 'dadjokes1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwNFPQdX0WpC"
      },
      "source": [
        "### Multiple Jokes to .txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO_0gXYXrrkV"
      },
      "source": [
        "Our final parameters were chosen as:\n",
        "* *length = 100* - maximum length of each joke (max is 1023)\n",
        "* *temperature = 1.2* - higher temp means more original jokes and less copies (default is 0.7)\n",
        "* *nsamples = 100* - number of jokes to be generated\n",
        "* *batch_size = 20* - generates multiple samples in parallel to speed up runtime (max is 20)\n",
        "\n",
        "We also added the below parameters that GPT-2 recognizes to handle the single-line output:\n",
        "* *prefix = \"<|startoftext|>\"* - GPT-2's recognized indicator for starting text\n",
        "* *truncate = \"<|endoftext|>\"* - truncates after each joke\n",
        "* *include_prefix = False* - removes prefix from output\n",
        "\n",
        "To download a .txt file of our output joke, we included the output file as :\n",
        "* *gen_file = 'gpt2_jokes.txt'* - can change file name\n",
        "\n",
        "Additional parameters for output file:\n",
        "* *destination_path = gen_file* - output .txt path\n",
        "* *sample_delim = ''* - removes \"====\" from in between each joke\n",
        "\n",
        "These parameters were chosen to output jokes with the most originality and highest scores. Details on how we optimized these parameters and scored our jokes are included in our final paper. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtYbb0xAve_k"
      },
      "source": [
        "gen_file = 'gpt2_jokes.txt'\n",
        "\n",
        "gpt2.generate_to_file(sess, run_name = 'dadjokes1',\n",
        "                      destination_path=gen_file,\n",
        "                      length=100,\n",
        "                      temperature=1.2,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20,\n",
        "                      prefix=\"<|startoftext|>\",\n",
        "                      truncate=\"<|endoftext|>\",\n",
        "                      include_prefix=False,\n",
        "                      sample_delim=''\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riLNoWSf29n0"
      },
      "source": [
        "The following command will prompt the file to be downloaded by Google Chrome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JrMpnRQEwFV6",
        "outputId": "503c03b8-e3fb-4a60-d418-e67dd43ba6e6"
      },
      "source": [
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f9d5d5a4-7880-4639-a504-d3d7740d1c08\", \"gpt2_jokes.txt\", 7261)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj4-HwDU0eDg"
      },
      "source": [
        "### Single Joke to Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGRZlOYFwNSU"
      },
      "source": [
        "To output a single joke or group of jokes to the notebook, run the following with the desired number of jokes (nsamples). If outputting >10 jokes, increase the batch_size to improve runtime. The parameters *length = 100* and *temperature = 1.2* can be changed for different joke lengths and joke originality. Note that a higher temperature will result in more original jokes but also more nonsensical output. A lower temperature will result in less original jokes. The given parameters are optimized for most originality with the highest score. Details on how we scored our jokes are included in our final paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvhN8ukIwKWY",
        "outputId": "7212391b-c82e-4252-ebdb-8bd36f4cee0d"
      },
      "source": [
        "gpt2.generate(sess, run_name='dadjokes1',\n",
        "              length=100,\n",
        "              temperature=1.2,\n",
        "              nsamples=10,\n",
        "              batch_size=1,\n",
        "              prefix=\"<|startoftext|>\",\n",
        "              truncate=\"<|endoftext|>\",\n",
        "              include_prefix=False\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Did the orange win the prize? It got the trophy.\n",
            "====================\n",
            "What do traditional sausages contain that can hold their shape? Yogscorns.\n",
            "====================\n",
            "How did high school introduce me? I was a purely sedentary mouse.\n",
            "====================\n",
            "Do you understand the number 15? He takes three seconds to type himself.\n",
            "====================\n",
            "Some flowers fight? Bison.\n",
            "====================\n",
            "Have you considered dropping by a pig and meeting another pig? Because there's not a lot of money.\n",
            "====================\n",
            "I made a playlist for hiking. It has music from band the aphas, and Muse. I call it my Trail Mix.\n",
            "====================\n",
            "The word queue is ironic. It's just q with a bunch of silent letters waiting in line.\n",
            "====================\n",
            "How did Greek anger crop up in the news? I don't know but it ain't nothing special.\n",
            "====================\n",
            "Where does Arnold Schwarzenegger hate the most? Cincinnati.\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ-rIENx669H"
      },
      "source": [
        "## Thanks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb7pCn1a7CGn"
      },
      "source": [
        "Thanks to [Max Woolf](https://minimaxir.com/) for his blogs and interactive notebooks on using GPT-2!"
      ]
    }
  ]
}